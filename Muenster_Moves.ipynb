{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3991d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install geopandas first, if not in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fa481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify and remove duplicates \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "df = pd.read_csv(r'E:/PROFESSION/NG/Research/PN_Raja/Muenster_hai.csv')\n",
    "duplicates = df.duplicated()\n",
    "print(df[duplicates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3497d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify and remove duplicates\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "df = pd.read_csv('hai.csv')\n",
    "duplicates = df.duplicated()\n",
    "print(df[duplicates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify missing data\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#a. Delete null\n",
    "# To drop rows with any missing values\n",
    "#df_cleaned = df.dropna()\n",
    "\n",
    "# To drop columns with any missing values\n",
    "#df_cleaned = df.dropna(axis=1)\n",
    "\n",
    "#b. Impute null\n",
    "# Impute missing values with mean\n",
    "#df_cleaned = df.fillna(df.mean())\n",
    "\n",
    "# Impute missing values with forward-fill\n",
    "#df_cleaned = df.fillna(method='ffill')\n",
    "\n",
    "# Impute missing values with backward-fill\n",
    "#df_cleaned = df.fillna(method='bfill')\n",
    "\n",
    "#######################  OR  ####################\n",
    "\n",
    "\n",
    "# Removing invalid rows\n",
    "#df_cleaned = df.drop(invalid_rows.index)\n",
    "\n",
    "# Imputing missing or invalid values in numeric columns\n",
    "#df['numeric_column'].fillna(df['numeric_column'].mean(), inplace=True)\n",
    "\n",
    "# Imputing missing or invalid values in categorical columns\n",
    "#df['categorical_column'].fillna(df['categorical_column'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0198c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explore outliers\n",
    "#a. Remove them\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'column_name' is the column containing outliers\n",
    "Q1 = df['ACTIVITY_INDEX_TOTAL'].quantile(0.25)\n",
    "Q3 = df['ACTIVITY_INDEX_TOTAL'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "\n",
    "# Filter outliers using the interquartile range (IQR) method\n",
    "#df_cleaned = df[(df['ACTIVITY_INDEX_TOTAL'] >= Q1 - 1.5 * IQR) & (df['ACTIVITY_INDEX_TOTAL'] <= Q3 + 1.5 * IQR)]\n",
    "#df_cleaned.head()\n",
    "\n",
    "#b. Transforming them using log transformation, square root transformation, or Box-Cox transformation\n",
    "# Log transformation\n",
    "#df['transformed_log'] = np.log(df['ACTIVITY_INDEX_TOTAL'])\n",
    "\n",
    "# Square root transformation\n",
    "#df['transformed_sqrt'] = np.sqrt(df['ACTIVITY_INDEX_TOTAL'])\n",
    "\n",
    "# Box-Cox transformation (requires data to be positive, so you might need to add a constant if necessary)\n",
    "from scipy.stats import boxcox\n",
    "#df['transformed_boxcox'], _ = boxcox(df['ACTIVITY_INDEX_TOTAL'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ed135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correct irrelevant data: Columns with constant values, Columns with mostly missing data, \n",
    "### and Data points or rows with missing values in critical fields.\n",
    "# Removing rows with missing values in critical fields\n",
    "df_cleaned = df.dropna(subset=['ACTIVITY_INDEX_TOTAL'])\n",
    "\n",
    "# Removing columns with constant values or high missing value percentage\n",
    "#df_cleaned = df.drop(columns=['column_with_constant_values', 'column_with_high_missing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Detect invalid or inconsistent data\n",
    "\n",
    "# Step 1: Identify Invalid or Inconsistent Data Start by thoroughly examining your dataset and identifying potential issues, \n",
    "# such as: Data outside valid ranges, Contradictory information, Format discrepancies, Data type inconsistencies.\n",
    "\n",
    "# Step 2: Data Validation Perform data validation checks to verify the integrity of the data. Common validation checks include:\n",
    "# Ensure that numeric values fall within valid ranges, Validate that data entries match the expected format, Cross-check related \n",
    "# fields to ensure their consistency.\n",
    "\n",
    "# Range checks for numeric columns\n",
    "#invalid_numeric_data = df[(df['ACTIVITY_INDEX_TOTAL'] < lower_bound) | (df['ACTIVITY_INDEX_TOTAL'] > upper_bound)]\n",
    "\n",
    "# Format checks for date column (assuming 'date_column' is in 'datetime' format)\n",
    "#invalid_date_format_data = df[~df['AGG_DAY_PERIOD'].dt.strftime(date_format).isin(valid_date_formats)]\n",
    "\n",
    "# Consistency check for related fields\n",
    "#inconsistent_data = df[df['related_field1'] != df['related_field2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75864e1e",
   "metadata": {},
   "source": [
    "### Data Exploration ###\n",
    "\n",
    "#### 1. Calculate summary statistics such as mean, median, and mode to understand the central tendencies and distributions of the data. ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dded480",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mean\n",
    "mean_value = df['ACTIVITY_INDEX_TOTAL'].mean()\n",
    "\n",
    "### Median\n",
    "median_value = df['ACTIVITY_INDEX_TOTAL'].median()\n",
    "\n",
    "### Mode\n",
    "mode_value = stats.mode(df['ACTIVITY_INDEX_TOTAL'], keepdims = True)[0][0]\n",
    "\n",
    "print(f'Mean is : {mean_value}, Median is : {median_value}, and Mode is : {mode_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb890d",
   "metadata": {},
   "source": [
    "#### 2. Visualize the data using graphs, charts, and histograms. This will help to identify any patterns or anomalies in the data. ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatter plot: Ideally this should be AGG_DAY_PERIOD Vs ACTIVITY_INDEX_TOTAL\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=df, x='XLON', y='XLAT', color = 'b')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Scatter Plot of Geographical Distribution of Human Activity Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Line Chart\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(df['AGG_DAY_PERIOD'], df['ACTIVITY_INDEX_TOTAL'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Activity Index')\n",
    "plt.title('Line Chart depicting human activity index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2233f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df['ACTIVITY_INDEX_TOTAL'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Activity Index Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of human activity index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67135d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Box Plot\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.boxplot(data=df, y='ACTIVITY_INDEX_TOTAL')\n",
    "plt.ylabel('Activity Index Value')\n",
    "plt.title('Box Plot of Human Activity Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbefc8",
   "metadata": {},
   "source": [
    "#### 3. Examine the correlations between variables using correlation matrices or scatter plots. ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b542fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation Matrix\n",
    "correlation_matrix = df.corr(numeric_only = True)\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4c4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Heatmap\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.density_mapbox(df, lat = 'XLAT', lon = 'XLON', z = 'ACTIVITY_INDEX_TOTAL',\n",
    "                        radius = 8,\n",
    "                        center = dict(lat = 49, lon = 12),\n",
    "                        zoom = 8,\n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color_continuous_scale = 'rainbow',\n",
    "                        opacity = 0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845afea",
   "metadata": {},
   "source": [
    "#### 4. Use hypothesis tests to form initial insights and generate new hypotheses. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43949f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918fba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba0dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ecaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the shapefile as a GeoDataFrame\n",
    "shp = gpd.read_file(r\"E:\\PROFESSION\\NG\\Research\\PN_Raja\\DEU_adm\\DEU_adm1.shp\")\n",
    "\n",
    "# Load the CSV file as a DataFrame\n",
    "#csv_df = pd.read_csv('path_to_csv.csv')\n",
    "\n",
    "# Convert the CSV DataFrame to a GeoDataFrame with point geometry\n",
    "csv_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.XLON, df.XLAT), crs='EPSG:4326')\n",
    "\n",
    "# Perform the spatial join\n",
    "joined_gdf = gpd.sjoin(csv_gdf, shp, how='left', predicate='intersects')\n",
    "\n",
    "# Create Choropleth map based on provinces\n",
    "#fig = px.choropleth_mapbox(\n",
    "#    data_frame = joined_gdf.set_index(\"ID_1\"),    # Using the id as index of the data\n",
    " #   geojson = joined_gdf.geometry,                # The geometry\n",
    "  #  locations = joined_gdf.index,                 # The index of the data\n",
    "   # color = 'NAME_1',\n",
    "    #mapbox_style = 'open-street-map',\n",
    "    #center = dict(lat = 49, lon = 12),\n",
    "    #zoom = 4)\n",
    "\n",
    "#fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1de6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382453a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import plotly.express as px\n",
    "\n",
    "shp = geopandas.read_file(r\"E:\\PROFESSION\\NG\\Research\\PN_Raja\\DEU_adm\\DEU_adm1.shp\")\n",
    "shp = shp.to_crs(\"WGS84\")\n",
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "    data_frame = shp.set_index(\"ID_1\"), # Using the id as index of the data\n",
    "    geojson = shp.geometry,                # The geometry\n",
    "    locations = shp.index,                 # The index of the data\n",
    "    color = 'NAME_1',\n",
    "    mapbox_style = 'open-street-map',\n",
    "    center = dict(lat = 49, lon = 12),\n",
    "    zoom = 4)\n",
    "\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175ea2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef452218",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pysal#\n",
    "#!pip install libpysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import libpysal as ps\n",
    "import esda\n",
    "\n",
    "# create a spatial weights object\n",
    "w = ps.weights.Queen.from_dataframe(joined_gdf, use_index = True)\n",
    "\n",
    "# display the weights object\n",
    "#print(w)\n",
    "\n",
    "# compute Moran's I\n",
    "mi = esda.moran.Moran(joined_gdf['ACTIVITY_INDEX_TOTAL'], w)\n",
    "\n",
    "# display the Moran's I statistic\n",
    "print(mi.I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a81bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysal\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "#data = pd.read_csv('path/to/your/csvfile.csv')\n",
    "\n",
    "# Create a spatial weights object\n",
    "w = pysal.weights.Queen.from_dataframe(df)\n",
    "\n",
    "# Compute the Moran's I index\n",
    "i = pysal.Moran(data['human_activity_index'], w)\n",
    "\n",
    "# Print the Moran's I index\n",
    "print(i.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98193efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a53593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spatial_autocorrelation(df):\n",
    "    # Calculate the distance matrix between all points\n",
    "    distances = spatial.distance.pdist(df.geometry.values.data)\n",
    "    # Get the number of points\n",
    "    n = len(df)\n",
    "    # Create a matrix of weights based on the inverse distance between points\n",
    "    weights = np.array([[1.0 / np.sqrt(distances[i, j])\n",
    "                         for j in range(n)]\n",
    "                        for i in range(n)])\n",
    "    # Calculate the Moran's I statistic\n",
    "    moran_i = spatial.stats.moran(df.value, weights)\n",
    "    print(f'Moran\\'s I = {moran_i:.4f}')\n",
    "    # Calculate the p-value\n",
    "    p_value = 1 - spatial.stats.t_test(df.value, weights)\n",
    "    print(f'p-value = {p_value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_spatial_autocorrelation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8112fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7410733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5788e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
